#  RAG AI Agent

A complete **Retrieval-Augmented Generation (RAG)** system that allows you to **upload PDFs**, store their content as **semantic embeddings in Qdrant**, and then **ask natural language questions** whose answers are generated by **GPT-4o-mini** using only the relevant document context.

---

##  Overview

The **RAG AI Agent** implements a full end-to-end pipeline:

1. **Upload PDFs** through a Streamlit interface.  
2. **Chunk and embed** document text using OpenAI‚Äôs `text-embedding-3-large` model.  
3. **Store** embeddings in a local **Qdrant** vector database.  
4. **Retrieve** relevant text chunks for user queries.  
5. **Generate answers** using GPT-4o-mini with context retrieved from the vector store.  

This project demonstrates a clean, modular RAG workflow built with modern AI tools.

---

##  RAG Pipeline Flow

```text
 PDF Upload
   ‚Üì
 Chunking + Embedding (OpenAI)
   ‚Üì
 Vector Storage (Qdrant)
   ‚Üì
 Query Embedding + Similarity Search
   ‚Üì
 Context-aware Answer (GPT-4o-mini)

```

---

## üõ†Ô∏è Tech Stack

| Component | Technology |
|------------|-------------|
| **Frontend** | Streamlit |
| **Backend API** | FastAPI |
| **Workflow Orchestration** | Inngest |
| **Embeddings** | OpenAI `text-embedding-3-large` |
| **LLM Answering** | GPT-4o-mini |
| **Vector Database** | Qdrant |
| **Chunking** | LlamaIndex SentenceSplitter |
| **PDF Parsing** | LlamaIndex PDFReader |
| **Environment Management** | python-dotenv |
| **Containerization** | Docker (for Qdrant) |

---




---

## ‚öôÔ∏è Installation & Setup

### 1Ô∏è‚É£ Clone the repository
```bash
git clone https://github.com/yourusername/rag-ai-agent.git
cd rag-ai-agent
```

### Create a virtual environment
```bash
python -m venv venv
source venv/bin/activate   # (Linux/macOS)
venv\Scripts\activate      # (Windows)
```
### Install dependencies
```bash
pip install -e .

```
### üîë Environment Variables

Create a .env file in the project root:
```bash
OPENAI_API_KEY=your_openai_api_key
INNGEST_API_BASE=http://127.0.0.1:8288/v1
```
### Run Qdrant (Vector Database)

You must run a local Qdrant instance before using the app.

Using Docker:
```bash
docker run -d \
  --name qdrantRagDb \
  -p 6333:6333 \
  -v ./qdrant_storage:/qdrant/storage \
  qdrant/qdrant
```
### Run the Backend (FastAPI + Inngest)

Start the backend server:
```bash
uvicorn main:app --reload --port 8288
```

This runs your Inngest functions and FastAPI endpoints locally.
Run the Frontend (Streamlit)

### Launch the Streamlit UI:
```bash
streamlit run streamlit_app.py
```

Then open ```http://localhost:8501```
 in your browser.
